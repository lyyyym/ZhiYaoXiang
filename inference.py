import os
import glob
import numpy as np

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["MINDNLP_BACKEND"] = "ms"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import mindspore as ms
from mindnlp.transformers import AutoModelForCausalLM, Qwen2Tokenizer
from mindnlp.peft import PeftModel
from sentence_transformers import SentenceTransformer
from myDB import SimpleVectorDB

# ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend", device_id=0)
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=0)
# =================é…ç½®åŒºåŸŸ=================
# 1. åŸå§‹åŸºåº§æ¨¡å‹
BASE_MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
LORA_DIR = "/root/mindnlp_project2/output_qwen_lora_inventory2"
SIMPLE_DB_DIR = "/root/mindnlp_project2/vector_store2"
EMBED_MODEL_NAME = "moka-ai/m3e-base"
# =========================================

print("â³ æ­£åœ¨åŠ è½½ Tokenizer...")
tokenizer = Qwen2Tokenizer.from_pretrained(BASE_MODEL_ID)

print("â³ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ (FP16)...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype="float16"
)

print("â³ æ­£åœ¨åŠ è½½å‘é‡åº“...")
vdb = None
meta_path = os.path.join(SIMPLE_DB_DIR, "meta.json")
vec_path = os.path.join(SIMPLE_DB_DIR, "vectors.npy")
if os.path.exists(meta_path) and os.path.exists(vec_path):
    vdb = SimpleVectorDB.load(SIMPLE_DB_DIR)
    embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")
else:
    print("âš ï¸ æœªæ‰¾åˆ°å‘é‡åº“ï¼Œå°†ä¸ä½¿ç”¨ RAG æ£€ç´¢ã€‚")

print("â³ æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
target_lora_path = None
if os.path.exists(os.path.join(LORA_DIR, "adapter_config.json")):
    target_lora_path = LORA_DIR
else:
    checkpoints = glob.glob(os.path.join(LORA_DIR, "checkpoint-*"))
    if checkpoints:
        target_lora_path = max(checkpoints, key=os.path.getctime)

if target_lora_path is not None and os.path.exists(target_lora_path):
    print(f"â³ æ­£åœ¨åŠ è½½ LoRA æƒé‡: {target_lora_path} ...")
    model = PeftModel.from_pretrained(model, target_lora_path)
else:
    print("âš ï¸ æœªæ‰¾åˆ° LoRA æƒé‡ï¼Œå°†ä½¿ç”¨åŸºåº§æ¨¡å‹æ¨ç†ã€‚")

model = model.to('npu:0')
model.set_train(False)

def retrieve_knowledge(query, top_k=3):
    if vdb is None:
        return ""
    def _embed(text: str):
        vec = embed_model.encode([text], normalize_embeddings=True)
        return np.asarray(vec[0], dtype=np.float32)
    results = vdb.search_text(query, _embed, top_k=top_k)
    parts = []
    for i, r in enumerate(results, 1):
        payload = r["payload"]
        name = payload.get("è¯å“åç§°") or payload.get("é€šç”¨åç§°") or ""
        indication = payload.get("é€‚åº”ç—‡", "")
        usage = payload.get("ç”¨æ³•ç”¨é‡", "")
        contraind = payload.get("ç¦å¿Œ", "")
        notes = payload.get("æ³¨æ„äº‹é¡¹", "")
        content = (
            f"è¯å“åç§°ï¼š{name}\n"
            f"é€‚åº”ç—‡ï¼š{indication}\n"
            f"ç”¨æ³•ç”¨é‡ï¼š{usage}\n"
            f"ç¦å¿Œï¼š{contraind}\n"
            f"æ³¨æ„äº‹é¡¹ï¼š{notes}"
        )
        parts.append(f"[è¯å“ {i}]:\n{content}")
    return "\n---\n".join(parts)

def generate_rag_response(query):
    """
    æ„é€ ç¬¦åˆè®­ç»ƒæ ¼å¼çš„ Prompt å¹¶ç”Ÿæˆå›ç­”
    """
    retrieved_context = retrieve_knowledge(query, top_k=3)
    if not retrieved_context:
        retrieved_context = "ï¼ˆæ— ç›¸å…³è¯å“ä¿¡æ¯ï¼‰"
    rag_prompt = f"è¯·å‚è€ƒä»¥ä¸‹è¯å“ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n### å‚è€ƒä¿¡æ¯å¼€å§‹ ###\n{retrieved_context}\n### å‚è€ƒä¿¡æ¯ç»“æŸ ###\n\nç”¨æˆ·çš„å…·ä½“é—®é¢˜æ˜¯ï¼š\n{query}"

    print("\n===== RAG Prompt (Training Format) =====")
    print(rag_prompt)
    print("===== End RAG Prompt =====\n")

    # 2. æ„é€ å®Œæ•´çš„å¯¹è¯æ¶ˆæ¯
    messages = [
        # System Prompt æœ€å¥½ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ— äººå”®è¯æœºçš„æ™ºèƒ½è¯å¸ˆåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºç»™å®šçš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æåŠç¦å¿Œç—‡æˆ–åº“å­˜ç¼ºè´§ï¼Œå¿…é¡»å‘å‡ºè­¦å‘Šæˆ–æ‹’ç»ã€‚"},
        {"role": "user", "content": rag_prompt}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="ms", padding=True)

    generated_ids = model.generate(
        model_inputs.input_ids.to('npu:0'),
        attention_mask=model_inputs.attention_mask.to('npu:0'),
        max_new_tokens=256,
        do_sample=True,
        top_k=20,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id
    )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return response

if __name__ == "__main__":
    user_query_1 = "æˆ‘ç‰™ç–¼å¾—å‰å®³ï¼Œè¿™æœºå™¨é‡Œæœ‰è¯èƒ½æ²»å—ï¼Ÿ"
    print("-" * 40)
    print(f"ç”¨æˆ·é—®é¢˜: {user_query_1}")
    print("ğŸ¤– æ¨¡å‹å›ç­” (Thinking...):")
    ans1 = generate_rag_response(user_query_1)
    print(ans1)

    user_query_2 = "æˆ‘æ˜¯å­•å¦‡ï¼Œç‰™ç–¼ï¼Œèƒ½åƒè¿™ä¸ªå¸ƒæ´›èŠ¬å—ï¼Ÿ"
    print("-" * 40)
    print(f"ç”¨æˆ·é—®é¢˜: {user_query_2}")
    print("ğŸ¤– æ¨¡å‹å›ç­” (Thinking...):")
    ans2 = generate_rag_response(user_query_2)
    print(ans2)

    user_query_3 = "æˆ‘æƒ³ä¹°ä¸ªåˆ›å¯è´´ã€‚"
    print("-" * 40)
    print(f"ç”¨æˆ·é—®é¢˜: {user_query_3}")
    print("ğŸ¤– æ¨¡å‹å›ç­” (Thinking...):")
    ans3 = generate_rag_response(user_query_3)
    print(ans3)
